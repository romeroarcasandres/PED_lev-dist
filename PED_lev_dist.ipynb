{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romeroarcasandres/PED_lev-dist/blob/main/PED_lev_dist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title-section"
      },
      "source": [
        "# CSV Column Comparison Tool\n",
        "\n",
        "This notebook provides a tool for comparing two columns in a CSV file, calculating similarity scores using Levenshtein distance, and generating both a modified CSV with scores and an HTML report showing the differences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-section"
      },
      "source": [
        "## Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-packages"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install python-Levenshtein diff-match-patch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import-section"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import-libraries"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import difflib\n",
        "import diff_match_patch as dmp_module\n",
        "import Levenshtein\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "helper-functions-section"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "helper-functions"
      },
      "outputs": [],
      "source": [
        "# Function to calculate the similarity score based on the raw Levenshtein distance.\n",
        "# It normalizes the raw distance so that if the strings are identical the score is 100,\n",
        "# and if they are very different the score will be closer to 0.\n",
        "def calculate_levenshtein_score(str1, str2):\n",
        "    # If both strings are empty, consider them identical\n",
        "    if not str1 and not str2:\n",
        "        return 100.0\n",
        "    # Calculate the raw Levenshtein distance\n",
        "    raw_distance = Levenshtein.distance(str1, str2)\n",
        "    # Normalize the distance using the length of the longer string\n",
        "    max_len = max(len(str1), len(str2))\n",
        "    similarity = 1 - (raw_distance / max_len)\n",
        "    # Multiply by 100 to convert to a percentage-like score\n",
        "    return similarity * 100\n",
        "\n",
        "# Function to calculate weights based on the length of the segments\n",
        "def calculate_weight(str1, str2):\n",
        "    return len(str1) + len(str2)\n",
        "\n",
        "# Function to generate HTML report of differences with row-level Levenshtein scores\n",
        "def generate_html_report(dmp, filename, col1_data, col2_data, diffs_list, score_column, report_name, header1, header2):\n",
        "    html_report = [\n",
        "        f'''\n",
        "        <html>\n",
        "        <head>\n",
        "            <style>\n",
        "                body {{\n",
        "                    font-family: Arial, sans-serif;\n",
        "                }}\n",
        "                table {{\n",
        "                    width: 100%;\n",
        "                    border-collapse: collapse;\n",
        "                }}\n",
        "                th, td {{\n",
        "                    border: 1px solid #dddddd;\n",
        "                    text-align: left;\n",
        "                    padding: 8px;\n",
        "                }}\n",
        "                th {{\n",
        "                    background-color: #f2f2f2;\n",
        "                }}\n",
        "                tr:nth-child(even) {{\n",
        "                    background-color: #f9f9f9;\n",
        "                }}\n",
        "                pre {{\n",
        "                    white-space: pre-wrap; /* Allows wrapping of long lines */\n",
        "                    word-wrap: break-word; /* Breaks long lines within the 'pre' tag */\n",
        "                }}\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "            <h2>Comparison Report for: {filename}</h2>\n",
        "            <table>\n",
        "                <tr>\n",
        "                    <th>Index</th>\n",
        "                    <th>{header1}</th>\n",
        "                    <th>{header2}</th>\n",
        "                    <th>Differences</th>\n",
        "                    <th>Score</th>\n",
        "                </tr>\n",
        "        '''\n",
        "    ]\n",
        "\n",
        "    for i, (data1, data2, diff_html, score) in enumerate(zip(col1_data, col2_data, diffs_list, score_column)):\n",
        "        html_report.append(f'<tr><td>{i + 1}</td><td>{data1}</td><td>{data2}</td><td><pre>{diff_html}</pre></td><td>{score:.2f}</td></tr>')\n",
        "\n",
        "    html_report.append('''\n",
        "        </table>\n",
        "        </body>\n",
        "        </html>\n",
        "    ''')\n",
        "\n",
        "    with open(f'{report_name}.html', 'w') as f:\n",
        "        f.write('\\n'.join(html_report))\n",
        "\n",
        "    print(f\"HTML diff report generated: {report_name}.html\")\n",
        "\n",
        "    # Download the HTML report in Colab\n",
        "    files.download(f'{report_name}.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "main-function-section"
      },
      "source": [
        "## Main Comparison Function (Modified for Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "main-function"
      },
      "outputs": [],
      "source": [
        "# Main function for comparison - adapted for Colab environment\n",
        "def compare_columns_in_csv():\n",
        "    # Upload a CSV file in Colab\n",
        "    print(\"Please upload a CSV file.\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Get the first uploaded file\n",
        "    csv_file = list(uploaded.keys())[0]\n",
        "\n",
        "    # Load CSV into pandas DataFrame\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Display available columns\n",
        "    print(\"Available columns:\")\n",
        "    for i, col in enumerate(df.columns):\n",
        "        print(f\"{i + 1}: {col}\")\n",
        "\n",
        "    # Prompt user to select columns using input() instead of tkinter\n",
        "    col1_index = int(input(\"Select the first column index (e.g., 1, 2, ...): \")) - 1\n",
        "    col2_index = int(input(\"Select the second column index (e.g., 1, 2, ...): \")) - 1\n",
        "\n",
        "    col1 = df.columns[col1_index]\n",
        "    col2 = df.columns[col2_index]\n",
        "\n",
        "    # Get the report name from the user\n",
        "    report_name = input(\"Enter the name for the HTML report (without extension): \")\n",
        "\n",
        "    # Initialize diff_match_patch and lists to store results\n",
        "    dmp = dmp_module.diff_match_patch()\n",
        "    diffs_list = []\n",
        "    score_column = []\n",
        "\n",
        "    col1_data = df[col1].astype(str).tolist()\n",
        "    col2_data = df[col2].astype(str).tolist()\n",
        "\n",
        "    for val1, val2 in zip(col1_data, col2_data):\n",
        "        str1 = str(val1) if pd.notna(val1) else \"\"\n",
        "        str2 = str(val2) if pd.notna(val2) else \"\"\n",
        "\n",
        "        # Calculate score using the normalized raw Levenshtein distance approach\n",
        "        score = calculate_levenshtein_score(str1, str2)\n",
        "        score_column.append(score)\n",
        "\n",
        "        # Generate HTML diff using diff_match_patch\n",
        "        diffs = dmp.diff_main(str1, str2)\n",
        "        dmp.diff_cleanupSemantic(diffs)\n",
        "        diff_html = dmp.diff_prettyHtml(diffs)\n",
        "        diffs_list.append(diff_html)\n",
        "\n",
        "    # Add the row-level score to the dataframe as a new column\n",
        "    df[f\"Score ({col1} vs {col2})\"] = score_column\n",
        "\n",
        "    # Save and download the modified CSV\n",
        "    output_csv = f\"modified_{csv_file}\"\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Modified CSV saved: {output_csv}\")\n",
        "    files.download(output_csv)\n",
        "\n",
        "    # Generate the HTML report with differences and row-level scores\n",
        "    generate_html_report(dmp, csv_file, col1_data, col2_data, diffs_list, score_column, report_name, col1, col2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "execution-section"
      },
      "source": [
        "## Execute the Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "execute-script"
      },
      "outputs": [],
      "source": [
        "# Run the comparison function\n",
        "compare_columns_in_csv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "optional-section"
      },
      "source": [
        "## Preview DataFrame (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preview-dataframe"
      },
      "outputs": [],
      "source": [
        "# Code to preview the DataFrame after processing\n",
        "# Uncomment and run this cell to see the results\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interactive-report-section"
      },
      "source": [
        "## Interactive Report Display (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "display-report"
      },
      "outputs": [],
      "source": [
        "# Display the HTML report in the notebook (requires the HTML file to exist)\n",
        "# from IPython.display import HTML, display\n",
        "# with open(f'{report_name}.html', 'r') as f:\n",
        "#     display(HTML(f.read()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}